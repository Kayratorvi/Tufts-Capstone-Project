<html lang="en">
    <head>
        <meta name="viewport" content="width=device-width" charset="utf-8" initial-scale="1">
        <title>Weekly Status Report</title>
    </head>
    <body>
        <h1>Gavin Grooms's Master's Capstone Project<br>Weekly Status Report</h1>
        <h2>Sept. 10-16 Report</h2>
        <p>
            This week we had our first meetup with Prof. Allen where we discussed the structure of this capstone project. I also began to brainstorm project ideas, as up to this point I hadn't locked down any concrete ideas for what to build. Instead of coming up with one idea, I wrote down three to consider that I submitted in my project proposal. Here are the ideas:
            <ol>
            <li>
                I want to develop an app that can re-tune accompaniment music to fit your custom tuning or can even re-tune music based on MIDI input automatically.
            </li>

            <li>
                I could build an application to meet the needs of an exchange student management company. The app would securely manage forms, data, and applications, etc. in an organized manner using feedback from my parents who work in that industry.
            </li>
            
            <li>
                I want to develop an app/discord bot that would help me manage my Pokémon collecting in a much more organized manner. Currently you have to use separate apps for tracking a hunt, managing your collection, and looking up information on how a Pokémon is caught or the odds of catching a particular one. This app would combine everything in a seamless interface
            </li>
            </ol>
            Each one of these ideas has something uniquely appealing to me, which is why I included all three. The first idea is clearly the most challenging, and would require algorithms and the use of languages and libraries I have little to no experience with. It could end up only as a proof of concept, but it is exciting and combines my two degrees, Music Performane and Computer Science, which is really cool.
            <br><br>
            The second idea is a full product that I would develop that has potential as a marketable application. It would require the use of a lot of languages and infrastructure to build, as ideally I would have both a web application and a mobile app that are connected.
            <br><br>
            The third idea is much more of a fun idea, as I would be able to build something just for me. To be honest, I may build it even if it isn't my capstone project. It would be fun to try and come up with several ways to interface with the data that I want to manage.
            <br><br>
            Hopefully I will receive feedback soon on what might help me most in helping me start my career as a software engineer. It really depends on whether having a finished product or a more complex project is more important. 
        </p>
        <h2>Sept. 17-23 Report</h2>
        <p>
            This week I have been working on deciding what my project will be of the three ideas I submitted last week. So far, the favorite has been project idea #1, which is both exciting and daunting. I have much less confidence in my ability to build this project than I do with the other two ideas. That is however a good thing, as success in a more difficult endeavor will surely be more impressive to potential employers.
            <br><br>
            The problem is that further research into what it would take to build this project has revealed that I would need to build two complex components that both work with each other. It is far more likely that I would only have time to do one of them.
            <ul>
                <li>I would need an intonation interpreter, something that can listen to a .wav or .mp3 file and determine the tuning of each pitch in the file. MIDI does not store tuning data whatsoever and thus would not be sufficient alone.</li>
                <li>The other component would be to build a software synthesizer that can play a MIDI file while automatically adjusting to the tuning input from a user or from the previous component. I looked into a programming API called CSound which may assist greatly in completing this particular task.</li>
            </ul>
            Either one of these components would be fairly complex. While it would definitely be cool to learn to build either one or both, I'm definitely apprehensive concerning the task(s) at hand. I have no knowledge on any of the tools I would need to use, and a significant portion of this project would be spent on learning to use them. If I chose instead to implement one of my other ideas, I would be able to get started much sooner, as there wouldn't be nearly as much I would need to learn from scratch. It would be great to get some feedback on my concerns and on which project idea I should move forward with.
        </p>
        <h2>Sept. 24-30 Report</h2>
        <p>
            This week I outlined my project requirements, including what problem this project is meant to solve, what my solution is, and a high-level explanation of how I intend to implement the solution. Much of this is well explained in the README of my GitHub repository, which can be found <a href="https://github.com/Kayratorvi/Tufts-Capstone-Project">here</a>.
            <br><br>
            I met with Marty Allen during our biweekly meeting and presented my project requirements and was asked some valuable questions:
            <ul>
                <li>How do I prove that my project does what I say it does to someone with limited experience in music and intonation?</li>
                <li>What can I do to ensure that my ambitious project idea can be delivered in some completed form in May 2024?</li>
                <li>What kind of input should I expect users to provide? Is it too much to ask for users to provide MIDI files as input?</li>
                <li>Will I expect a live recording handled by my application, or will I expect a specific recording file format?</li>
            </ul>
            These are all important questions. I can verify the project's authenticity by using third-party tuners and verification from musicians I know that my project indeed retunes chords as defined in my project spec. I have also defined programming phases in my README that should guarantee an impressive, usable product that can be delivered as my capstone project. I also think it's reasonable to expect MIDI input as that is a growing medium that many college-level musicians have access to through their university benefits and education. I intend to answer the question of human recording later as I investigate various solutions.
            <br><br>
            I have approval to move forward with this project. I'm excited to see whether I am capable of success in such a challenging endeavor. If not, I do have a plan B project proposal involving a custom app for foreign exchange student management companies. This idea is not as exciting, but should still allow me the opportunity of building an application using many different tools, languages, and technologies.
        </p>
        <h2>Oct. 1-7 Report</h2>
        <p>
            This week I focused on researching existing libraries, APIs, and frameworks if any exist that might help facilitate my project. I discussed my project idea with my manager at my workplace, and he suggested I look into Audacity, which is an open-source audio editor. As it turns out, Audacity has full support for scripting in the Python language, and looks to have full capability for use as the synthesizer for my project. What this means is that I should be able to complete the first phase of this project using Audacity fairly quickly, allowing me to pursue the later phases of my project instead of spending all my time building a synthesizer from scratch.
            <br><br>
            My next step is to practice using Audacity enough to prove that it is fully capable of supporting my capstone project. This will be submitted as my Competency Demo. My plan is to write a small program that can retune 5 notes fed in from MIDI input. If I can do this, then the entirety of my first phase should be very doable. This will be my task for the coming week or so.
        </p>
        <h2>Oct. 8-14 Report</h2>
        <p>
            This week I started working with Audacity in testing its scripting abilities to determine whether it could be used as an API for my capstone project. Unfortunately, I found that while Audacity can produce tones with custom pitches, it's very barebones and lacks much of the functionality I would need to rely on Audacity alone as a synthesizer. There is still potential for its use as a mixer in helping me combine multiple tracks (computer generated input and the live instrument recording) however it is not a suitable resource for producing more complex synth sounds.
            I have a few open source synthesizers to investigate as I researched further for other options, including Amsynth, Zyn, and Yoshimi among others. Ideally it would be great to find an application that would allow me to write scripts to produce synthesized music, but I may very well end up needing to write the synthesizer myself instead. I'll update next week with my findings.
        </p>
        <h2>Oct. 15-21 Report</h2>
        <p>
            This week I spent all my time researching alternatives for use as a backend library to generate the notes and tones I need to build my synthesizer. It turned out that Amsynth, Zyn, Yoshimi, and Audacity all do not provide the APIs I need to generate my own sound from my own input. They all in some way act as mixers and can modify existing audio, and as programs only offered high-level interfaces that do not provide the tools I need. I decided I had to look at much lower-level libraries to find what I needed. I came across a library known as Nsound, which itself is built as an API for Csound. This is a low-level library that offers the ability to generate tones to be used for a myriad of purposes. Since Nsound is released using the GNU license, I should be able to use and/or modify it to my needs. Based on what I have read into it so far, I will still need to write a program to read from a MIDI file and then use Nsound to generate the tones to my preference and tuning. Most likely, this means my project will primarily be to write the synthesizer that my later proposed phases would need to rely on. I still should be able to use Python as my language of choice, as my understanding is that I should be able to interact with C++ libraries easily using Python. This coming week I plan to look into writing an example program using Nsound to hopefully demonstrate that it has the programming interfaces that I need to make my project a reality.
        </p>
        <h2>Oct. 22-28 Report</h2>
        <p>
            This week I started working with NSound to produce the computer generated tones that I would need to make my project a reality. It has turned out that this library will work great for my needs! It has built-in instruments that I can use and methods for easily adjusting intonation and note length. It should be sufficient for my needs as part of the backend that will support my capstone project. The only trouble I've run into, is that NSound is limited by computer hardware in how many tones it can produce simultaneously. Modern computers generally support two audio channels, and this is what the OS allows. Natively producing more than two tones at once will require either a specialized OS and hardware, or simply the addition of a mixer Python library to overlay .wav files together. I plan to research options starting with Audacity, which I found to be a mixer at its core to see if I can write a Python script to produce the tones and overlay them to produce a single audio .wav output file.
        </p>
        <h2>Oct. 29-Nov. 4 Report</h2>
        <p>
            The focus of this week was to prepare a script that utilizes NSound and a mixer library to demonstrate the output of a chord with three tones in one .wav file. Initially, I was set back by a large project that was due in my Intro to Security class, but I ended up successful even if it wasn't in the way I expected. While Audacity is a mixer and would work for what I intend to do in overlaying audio together, it is more difficult and clunky to use, requiring that the application be installed and open. This is not ideal for an end product that I wish to be able to install on lots of devices. I researched other options and came across the popular Pydub Python library, which has support for making adjustments to audio files including overlaying them to output one file. I have decided to go with this library in the end since it is easier to package together and use with my own scripts and will likely allow for easier distribution of an end product than anything dealing with Audacity. My plan for the coming week is to formally complete my competency demo for presentation before starting on my full design document and proof of concept demo. 
        </p>
        <h2>Nov. 5-11 Report</h2>
        <p>
            This week I met with Marty Allen again and discussed how to handle the submission of my competency demo and proof of concept demo, as they have basically melded into the same thing. I don't have much experience with Python, NSound, or any of the tools I'm using for my project, so I went ahead and worked on a proof of concept to also serve as proof that I am capable of working on the project in the first place. I am submitting both on Nov. 12 to Canvas. Since Zoom wouldn't include the audio from the two .wav files created during my demo, I am instead going to embed them inside this week's entry. They should both be below, 261 hz and 440 hz:
            <br>
            <audio controls>
                <source src="./docs/gavin_test_chord_261.wav" type="audio/wav">
                Your browser does not support the audio tag.
            </audio>
            <br>
            <audio controls>
            <source src="./docs/gavin_test_chord_440.wav" type="audio/wav">
            Your browser does not support the audio tag.
            </audio>
            <br>
            My next task is to work on a full design document before getting started on the actual implementation. I should be able to start implementation before the semester ends. 
        </p>
        <h2>Nov. 12-18 Report</h2>
        <p>
            This week I began working on my full implementation document. I want to make sure it's full of detail and leaves nothing out, so I'll likely finish sometime in the next week or so. With Thanksgiving this next week, I'll likely take that week off and finish the design document the following week, the last week of November. This should position me so that I'm ready to tackle the coding part of this capstone project head on beginning in December. I'm feeling pretty excited about the shape my project has taken and about my progress so far. The proof of concept demo helped me to more fully envision what this project will look like when it is completed. 
        </p> 
        <h2>Nov. 26-Dec. 2 Report</h2>
        <p>
            This week I submitted my full implementation plan. This included detailed writeups for all phases of my project, and I outlined how I would approach the beginning and end of each phase, as well as described the goals for each phase. I also talked about the tools I would be using, such as the Python libraries I'm working with, the data structures I plan to use, and what kinds of things I'll work with to test my program at each step. This is the final deliverable for this first semester, so I will wait for feedback and improve upon my implementation document if it is needed.
        </p>
        <h2>Dec. 3-9 Report</h2>
        <p>
            This week I got sick with a terrible chest cold, so even though I received feedback requesting for more specific details on my data structures and functions, I was not able to do much work this week and spent most of my time in bed.
        </p>
        <h2>Dec. 10-16 Report</h2>
        <p>
            This is the final week I plan to report on until after the New Year, as I plan to take the holiday time off to be with my family. This week I updated my implementation document with exact plans for each of my planned data structures, even going so far as to provide the code I would use to build each one. I also wrote a few Python functions to demonstrate how I plan to get my project to work in the first phase. This was accepted, so my deliverables for this first half of my capstone project are now completed to satisfaction. After the holiday break, I will begin work in earnest on the actual implementation itself. Merry Christmas and Happy New Year!
        </p>
        <h2>Jan. 21-27 Report</h2>
        <p>
            This is the start of my reporting for the second semester of my capstone project! Due to my completion of all planning aspects of this project before the holiday break, I elected to take the holiday as a break from all school-related activities, so I do not have anything to report on for that time. This week I worked on setting up a plan for an exact order of my project for phase 1. First, I intend to implement my MIDI parser functionality, as that will potentially impact how the rest of phase 1 will be implemented. The parser should be completed ideally by Feb. 10th so that I can begin work on translating the data pulled from the parser into the audio output. I started work on this late this week, and will have more to report on in this coming week's report.
            <br><br>
            I discovered that the MIDIFile python library isn't easy to work with on Windows, which is the OS I am using. I am instead going to try and work with a different MIDI library I found, called Mido.
        </p>
        <h2>Jan. 28-Feb. 3 Report</h2>
        <p>
            This week I was able to successfully get Mido included in my project to read in a MIDI file. However, I found in investigating a MIDI file I read using Mido that MIDI files are structured a bit differently than I had hoped. Essentially, a MIDI file contains an array of MIDI "messages" that are interpreted by a MIDI capable device as commands in live time. NSound will be looking for input that specifies the exact length of each note to be played, which means I will need to perform some preprocessing on the MIDIMessages to specify the length of time each note is to be played. This will need to be done by reading through the entire MIDI file and putting the end_note timestamp on the end of each note_on message when a message is found that turns the note off. Because the structure provided by Mido doesn't include this functionality on its own, I will need to build my own data structure to handle the extra data. 
            <br><br>
            MIDI messages use delta time in microseconds to determine how much time to wait in between MIDI messages. So, this new data field will be the total time in microseconds from the time the note_on message starts to the time its corresponding note_off message is found. The data structure will replace the MIDIMessage and I will plan to create an array of these modified messages. Since I will likely need somewhere to store my custom intonation frequencies during preprocessing, I will also add a field to store the frequency as a number.
            <br><br>
            Once the preprocessing is complete, I should be able to more directly feed each MIDI message into NSound, as each one will have information on the total length of the note as well as its final intonation. This should allow for a more seamless stream from Mido to NSound. 
            <br><br>
            While this week did not include too much coding aside from setting up Mido, it did include a lot of research and planning to come up with the above. Next week I plan to start writing the code needed for preprocessing the note_off timestamp.
        </p>
        <h2>Feb. 4-10 Report</h2>
        <p>
            This week I began work on writing the data structure to extend the MidiMessage structure, and also worked on my first attempt at parsing through the MIDI file to add all note_on messages to a list using my data structure to add the length of each note. At this point, I need to perform some testing on what I have so far, because I may already have both solutions exactly as they should be.
            <br><br>
            My data structure is called MidiMessageExt and adds the fields timestamp, note_length, and frequency. The timestamp is the total of all delta time up to the start of this message, while the note_length field is the difference between the corresponding note_off timestamp and this message's timestamp. I plan to use the frequency field later on to store information I need to accurately adjust the tuning of each note as planned. 
            <br><br>
            The function I wrote is a loop that parses every message in the MidiFile, ignoring all messages (for now) that do not have the type note_on or note_off. If it's a note_on message with a velocity greater than 0, then the message is extended using my custom data structure and is added to another list with a value of -1 as the note_length field and the current timestamp in the timestamp field. Then, later on when a note_off or note_on with velocity 0 message is found, it searches from the beginning of my list of extended messages for a note_on message with a matching note and channel that does not have a note_length assigned to it yet and fills in that field. This ensures that once a note has a note_length assigned to it that any later note_off message with the same channel and note ignores this message as it looks to add a note_length field. Please see my capstone_testing.py file for a look at what this function looks like. 
            <br><br>
            This function was successful when I plugged in a large MIDI file I found online. However, MIDI files can vary a lot so I intend to spend some time testing this function in the coming week before moving onto further preprocessing steps to make sure that this function is working exactly as intended. This function is foundational in my project, so it's critical that I have the implementation exactly right.
        </p>
        <h2>Feb. 11-17 Report</h2>
        <p>
            As mentioned in my report from the previous week, I spent time this week gathering various MIDI files and testing them with my program thus far. My function is designed to throw an exception upon failure, and in all my tests everything worked as expected. It seems safe at this point to continue with the next step of this phase, since as far as I can tell everything is working correctly.
            <br><br>
            The next step is to write some additional preprocessing code to link all the MidiMessageExts together in a logical format. This will require that I use the ticks_per_beat value of the MIDI to group notes together with other notes that are nearby. I plan to do this by adding a list to each MidiMessageExt of all notes that end within one beat of the current note or are still on or start within a beat of the current note. The idea will be that each note on its own will know all the other notes that are being played at the same time so that analysis of chord progressions can be performed in a way that makes sense in a separate preprocessing step. Linking all notes together is critical to ensure this is possible. Hopefully I will have the function to link all notes to their neighbors finished by the end of next week's report.
        </p>
        <h2>Feb. 18-24 Report</h2>
        <p>
            This week I began work on writing the additional preprocessing that will add all notes that belong together to a list in each MidiMessageExt, which includes the actual MidiMessageExt and index in the list of each note. I'm pleased to write that my new function to handle this logic works correctly. It was tricky making sure that the function was efficient as I was potentially looking at O(n^2) time if I ran all parts of the function naively. It's way too time consuming to inspect every note in a piece of music when determining which ones are relevant to each other, so I tried to write the loops in a way that would skip notes that are irrelevant. This is accomplished by the fact that the list of MidiMessageExts that I have is ordered by timestamp. Any index greater than the current note either begins on or after the same timestamp as the current note, and any index less than the current note begins on or before the same timestamp as the current note. This meant that I could stop iterating as soon as I encountered a note that was too far away in timestamp in either direction.
            <br><br>
            However, I soon realized there was a critical error with this logic: suppose I am considering a quarter note with index 38 in measure 3 and have one instrument in an orchestra playing a prolonged note with index 0 that lasts 3 whole measures. This note at index 0 will be relevant to all notes that are played until the end of measure 3, but my logic above to save time may not consider this note if it stops iterating backwards due to a short note that begins and ends quickly in measure 2 at index 28. This meant I needed a way to somehow know that this note at index 0 was relevant without being required to iterate all the way back to index 0. In the worst case, I might have music for bagpipes that has a note being played from the beginning of the piece all the way through the end!
            <br><br>
            However, I have found a solution that maintains time savings while ensuring that these exceptionally long notes are not ignored. I created an extra loop after the backwards iteration to look for any notes that those notes are linked to that may be relevant to the current note being worked on. This works because in the above example, the notes at index 1 and 2 will obviously consider note 0, and note 3 will know to add note 0 since it is relevant to note 2 even if note 1 is not relevant!
            <br><br>
            I'm pretty happy with this solution, and in my testing the preprocessing runs even for long pieces without taking up too much extra time. I have ensured that these exceptional cases are correctly covered and that the list of relevant notes are correct for each note in the list.
            <br><br>
            This coming week I will begin a large step of my capstone, which will be to write rules to determine the actual intonation for each note before I can feed them into my Nsound stream.
        </p>
        <h2>Feb. 25-Mar 2 Report</h2>
        <p>
            This week I was unable to complete much work due to being sick and dealing with a stressful midterm exam in the other class I'm taking this semester. I hope to have more work completed next week towards my chord analysis logic.
        </p>
        <h2>Mar. 3-9 Report</h2>
        <p>
            This week I added some additional functions to the MidiMessageExt class to allow for easier preprocessing in the next step of my project, which will be to actually decide how notes should be tuned to each other. First, I wrote a function that separates linked notes into two lists: the first for notes that are actually playing at the same time as the current note, and the second for notes that are strictly nearby and not simultaneously playing. The purpose of this function is to ensure I can inspect notes that overlap at a higher importance level than notes that are only nearby but do not overlap. My intention is to place greater importance on notes that are being played together, to ensure those notes are played in tune with each other. However, seeing notes that are nearby will still be useful in some cases so I still want to have those for reference.
            <br><br>
            The other functionality I added this week was a function that returns a list of the unique notes from the linked notes and nearby notes lists. For example, one note I've been considering for my tests has over 20 notes linked and nearby, however running this function returns information that indicates that there are actually only 3 different notes being played in all the octaves. Information like that can help me easily identify that those three notes turn out to form a single simple chord, and that I should tune the notes accordingly.
            <br><br>
            Next week, I plan to begin work on my functionality to determine tuning on each individual note, as I believe I now have enough information from my preprocessing to start to write logic to make those determinations.
        </p>
        <h2>Mar. 10-16 Report</h2>
        <p>
            This week I started work on my tuning analysis functions. I wrote a lot of helper functions, including one that returns the distance between a root note and another note, one that checks if a note fits within a chord, and one that naively decides whether a chord can be identified from a group of notes. So far, everything works correctly! I plan to introduce more in-depth analysis for the corner cases where it is not clear what chord is playing. I think what I can do is try to identify cases where there is just one note that doesn't fit in cases of intentional dissonance. This could be done by taking cases where there are four or five note numbers existing at once and removing one from consideration and rerunning my existing simple chord analysis. Then, I could check for specific dissonance examples that make sense and "allow" those to exist. This should work very well for many cases.
            <br><br>
            I think I'm well on my way to completing this capstone project's first phase by the end of the semester! 
        </p>
        <h2>Mar. 17-23 Report</h2>
        <p>
            This week I made some tweaks to my program after I realized that I have a list of "nearby notes" that I am tracking that do not matter for my purposes at all. Originally I thought that I would need to make sure I can identify chords between notes that are not played together, but I realized that my true intention behind this project is to make sure that notes that are played together are tuned to each other, just as any modern orchestra would do. So, it doesn't actually matter whether notes that do not overlap are tuned to each other or not.
            <br><br>
            This definitely simplifies things a bit, in that I do not need to correct my program if it decides that an uncommon chord exists someplace where literally only one note is being played. A note like that should be tuned by itself anyways.
            <br><br>
            I spent some time cleaning up some code, adding notes, and commenting out some of the "nearby notes" functionality since I do not expect I'll need it anytime soon.
        </p>
        <h2>Mar. 24-30 Report</h2>
        <p>
            This week I started working on the logic that will handle identifying chords where there is a single dissonant note somewhere. I accomplished this by adding a new field to track whether a note is considered dissonant (so as to ensure I ignore it when considering tuning other notes) and by writing a few new cases to handle this logic. 
            <br><br>
            First, I adjusted the beginning logic in the set_tuning function by essentially checking whether a note is dissonant with the established (or to-be established) tuning. This is done simply by handling the failure of the is_chord function differently. Now, I have a catch that will trigger an if path that will identify a situation where a chord is already established, but a note doesn't fit inside that chord. I have decided to allow the following dissonant intervals: [2, 5, 11] as well as the minor or major sixth depending on whether the original chord is major or minor [8 or 9]. 
            <br><br>
            If a dissonance is identified, the note is assigned the same bass_tuning as all other notes in the chord in spite of the dissonance, and has the is_dissonance flag set to True. This flag is important, as it ensures that future notes that may be part of the same chord ignore this dissonant note when making their own decisions.
            <br><br>
            This seems to work correctly so far. Next week I will write a case to handle the situation of the dissonant note being the first one to have its tuning assigned since I realized that's totally possible.
        </p>
        <h2>Mar. 31-Apr. 6 Report</h2>
        <p>
            As mentioned last week, I worked this week on handling the case where I am trying to tune a chord that contains a dissonant note, but I have not yet tuned any notes in that chord. My current outcome is that nothing gets tuned, which is not ideal. I need a separate case from the one I wrote last week to ensure that when deciding what the tuning for the overall chord is for the first time, allowed dissonances are ignored.
            <br><br>
            This required a nested loop checking every combination of root note and dissonant note until the right combination is found, using the rules I wrote last week. Each dissonant note would be left out of the list of notes that are checked against a potential root of the chord until an appropriate root and allowed dissonant chord is found. 
            <br><br>
            This new case worked really well! I cut down the number of undefined tunings from 3500 to 1000 for the Horn Concerto MIDI I'm using for testing. The majority of undefined tunings left are cases of clusters of notes where a chord really cannot be defined very easily. I'm pretty happy with where I'm at in terms of what is tuned and what isn't. The plan now is to write the audio output file code so that I can start testing that everything I'm doing actually has the intended outcome. Assuming that goes well, the plan for the remainder of the semester will be to tweak this program further with small improvements until it is time to present. I may have time to introduce other MIDI features, including tempo and other stylistic pieces. 
        </p>
        <h2>Apr. 7-13 Report</h2>
        <p>
            This week I reached a critical milestone: my code now actually produces an audio file! It's clear from the audio files I've produced that my implementation thus far has been correctly done. I can hear the adjusted tuning, and it makes the music sound much better when chords are played. This is very exciting to have reached this step!
            <br><br>
            This ended up being quite the undertaking to produce audio. I underestimated how much it would take to get the audio stream working properly, and I'd like to still work out some kinks. It turned out that I needed to implement tempo tracking in my MidiMessageExt class, and that I needed a timestamp in seconds field that I could reference as well. Due to the possibility of any number of notes being played simultaneously, I needed to ensure that any number of channels in my final output were allowed. This meant I needed a way to add new channels in my audio stream on the fly, as I can't assume how many I will need just from the original MIDI file itself. It was a bit tricky getting everything to line up, but once I did I managed to overlay all channels over top of each other to produce my end result. To my delight, I could easily recognize the music being played from the .wav file! Everything synced up and I could tell that my intonations were coming through, though as expected it's subtle. 
            <br><br>
            It might be a challenge to convince Marty Allen that my program actually accomplishes something, so we'll see on Wednesday what he thinks about where I'm at now and whether he believes that my project accomplishes my original goal in the first phase or not.
        </p>
    </body>
</html>